import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# BaÅŸlÄ±k
st.set_page_config(page_title="JustiBot - Hukuk AsistanÄ±", layout="centered")
st.title("âš–ï¸ JustiBot - Hukuk AsistanÄ±nÄ±z")
st.write("EÄŸitilmiÅŸ yerel modeli kullanarak hukuki sorularÄ±nÄ±za yanÄ±t verir.")

# Model ve tokenizer'Ä± Ã¶nbelleÄŸe alarak yÃ¼kle
@st.cache_resource
def load_model():
    model_path = "tiny-lawbot-model"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")
    return tokenizer, model

tokenizer, model = load_model()

# KullanÄ±cÄ±dan giriÅŸ al
user_input = st.text_area("ğŸ” Sorunuzu yazÄ±n:", height=100, placeholder="Ã–rnek: Anayasa madde 2â€™ye gÃ¶re TÃ¼rkiye Cumhuriyetiâ€™nin temel nitelikleri nelerdir?")

if st.button("ğŸ§  YanÄ±tla"):
    if user_input.strip() == "":
        st.warning("LÃ¼tfen bir soru girin.")
    else:
        with st.spinner("Model yanÄ±tlÄ±yor..."):
            # Prompt formatÄ±
            prompt = f"Prompt: {user_input}\nResponse:"

            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                output = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )

            response = tokenizer.decode(output[0], skip_special_tokens=True)
            # CevabÄ±n yalnÄ±zca Response kÄ±smÄ±nÄ± ayÄ±klayalÄ±m
            if "Response:" in response:
                final_output = response.split("Response:")[-1].strip()
            else:
                final_output = response.strip()

            st.success("ğŸ—£ï¸ YanÄ±t:")
            st.write(final_output)
